### Rational Decisions
- Rational: Maximally achieving pre-defined goals
- Rationality only concerns what decisions are made (not the thought behind them)
- Goals are expressed as utility of outcomes
- *Being rational means maximising expected utility*

### About Brain
- Brains are good at making rational decisions, but not *perfect*
- Brains are not as modular as software, so hard to reverse engineer

### Designing Rational Agents
- Agent: Entity that *perceives* and *acts*
- Rational agent: who selects the actions that maximise **(expected) utility**
- Techniques for selecting rational actions: **percepts, environment, and action space**

![RationalAgent](/assets/RationalAgent.png)
- Agent *perceives* the environment through **sensors** and acts upopn it through **actuators** (or **effectors**)
- *Agent function* maps percept sequences to actions
- Generated by an *agent programme* runing on a *machine*
---
### Task environment - PEAS
PEAS refers to *Performance measure*, *Environment*, *Actuators*, and *Sensors*.
#### Example 1 - Pacman

![Pacman](/assets/Pacman.jpg)
- **Performance measure**
    - -1 per step; +10 food; +500 win; -500 die; +200 hit scared ghost
- **Environment**
    - Pacman dynamics (including ghost behaviour)
- **Actuators**
    - LRUD or NSEW
- **Sensors**
    - Entire state is visible (excluding power pellet duration)

#### Example 2 - Automated taxi

![AutomatedTaxi](/assets/AutomatedTaxi.png)
- **Performance measure**
    - Income; Happy customer; Vehicle costs; Fines; etc.
- **Environment**
    - Streets; Other drivers; Customers; Weather; etc.
- **Actuators**
    - Steering; Brake; Gas; etc.
- **Sensors**
    - Camera; Radar; GPS; etc.

#### Example 3 - Medical diagnosis system

![MDS](/assets/MDS.png)
- **Performance measure**
    - Patient health; Cost; Reputation
- **Environment**
    - Patients; Medical staff; Insurers; Courts
- **Actuators**
    - Screen Display; Email
- **Sensors**
    - Keyboard/mouse

### Environment Types
- Fully/Partially observable
- Single/Multi-agent
- Deterministic/Stochastic
- Static/Dynamic
- Discrete/Continuous
- Known physics?
- Known performance measure?
---
## Type of AI Agents
### Simple reflex agents
A simple reflex agent is the simplest of all the agent programs.
- Simple Reflex agents take decisions on basis of current percepts, and ignore the rest of percept history.
- Only succeed in the *fully observable environment*.
- Works on condition-action rules.
 
This kind of connection where only one possibility is acted upon is called a condition-action rule, written as: *"**if** hand is in fire **then** pull away hand"*.

![SimpleReflexAgent](/assets/SimpleReflexAgent.jpg)
### Reflex agents with state (Model-based reflex agent)
The Model-based agent can work in a partially observable environment, and track the situation.
- A model-based reflex agent have **model** and **internal state**
    - Model: the knowledge about "how things happen in the world"
    - Internal State: a representation of the current state based on percept history.
- It runs by keeping the internal state that depends on what it has seen before so it holds information on the unobserved aspects of the current state.
- To update the agent state, information about "How the world evolves" and "What my actions do" are required.
    - For example, a Mars Lander after picking up its first sample, it stores this in the internal state of the world around it so when it comes across the second same sample it passes it by and saves space for other samples. In order to update the internal store, we need to know *"If our mars Lander picked up the rock next to the one it was going to the world around it would carry on as normal"* (**How the world evolves**) and *"If our mars Lander took a sample under a precarious ledge it could displace a rock and it could be crushed."* (**What my actions do**)

![ReflexAgentswithState](/assets/ReflexAgentswithState.png)

### Goal-based agents
When the knowledge of the current state environment is not always sufficient to decide for an agent to what to do, the agent needs to know its goal which describes desirable situations.

> *A simple example would be the shopping list; our goal is to pick up every thing on that list. This makes it easier to decide if you need to choose between milk and orange juice because you can only afford one. As milk is a goal on our shopping list and the orange juice is not we chose the milk.*

- Goal-based agents expand the capabilities of the model-based agent by **having the "goal" information**
- Unlike the previous reflex agents before acting this agent reviews many actions and chooses the one which come closest to achieving its goals, whereas the reflex agents just have an automated response for certain situations.
- Pro: GBA use goal to index into actions that might achieve it
- Con: GBA cannot handle trade-offs among goals, failure probability, etc.

![GoalbasedAgents](/assets/GoalbasedAgents.png)

---

### Utilities
**Utilities** are functions from outcomes (states of the world) to real numbers that describe an agent's preferences.

When we say principle of maximum expected utility, it shows that a *rational agent* should choose the action that **maximises its expected utility, given its knowledge**.

![Utility](/assets/Utility.png)

#### Where do utilities come from?
- In a game, utilities may be simple (+1/-1)
- Utilities summarise the agent's goals
- Any "rational" preferences can be summarised as a utility function

**Uncertain outcomes** may also reflect different utilities as results of different choices.

![UtilitywithUncertianOutcomes](/assets/UtilitywithUncertianOutcomes.png)

### Perferences
- An agent must have preferences among:
    - **Prizes**: $A$, $B$, etc.
    - **Lotteries**: situations with uncertain prizes, notated as $L=[p,A;(1-p),B]$
- We use notation:
    - **Preference**: $A>B$
    - **Indifference**: $A\sim B$
![PrizeandLottery](/assets/PrizeandLottery.png)

### Axioms of Rationality
- **Orderability**:
$$(A>B) \lor (B>A) \lor (A\sim B)$$
- **Transitivity**:
$$(A>B)\land (B>C) \implies (A>C)$$
- **Continuity**:
$$(A>B>C)\implies \exists p \;[p,A;1-p,C]\sim B$$
- **Substituability**:
$$(A\sim B)\implies [p,A;1-p,C]\sim [p,B;1-p,C]$$
- **Monotonicity**:
$$(A>B)\implies (p\geq q) \iff [q,A;1-q,B]$$



### Maximum expected utility (MEU) principle:
The principle of maximum expected utility states that a rational agent should select the move that maximizes the agentâ€™s expected utility. In other words, the MEU principle is a prescription for intelligent behavior.
#### Theorem [Ramsey, 1931; von Neumann & Morgenstern, 1944]
Given any preferences satisfying these constrains, there exists a real-valued function $U$ such that:

$$U(A) \geq U(B) \iff A \geq B$$

$$U([p_1,S_1;\dots;p_n,S_n])=p_1U(S_1)+\dots+p_nU(S_n)=\Sigma_{i=1}^{n} p_iU(S_i) $$
*I.e., values assigned by $U$ preserve preferences of both prizes and lotteries.*

>To illustrate, consider the following example. I need to find out whether I need to carry an umbrella? Given that, If it rains and I have no umbrella, my utility is 0 units, while it is 15 units if I have an umbrella. If it does not rain and I have no umbrella, my utility is 18 units, while it is 15 units if I do have an umbrella. The chances of rain on any day is 50%.
![meu](/assets/meu.png)
In the above example, we calculated the EU for both cases, and according to MEU, the EU of the left part is higher; therefore, I should carry the umbrella.

### Utility of Money
Money **does not** behave as a utility function, but we can discuss the utility of *having money*.

The almost universal exchangeability of money for all kinds of goods and services implies that money plays an essential role in human utility functions. If all other things are equal, an agent prefers more money to less in all the scenarios. We say that the agent exhibits a monotonic preference for more money.

Consider these two cases:

![CASE1](/assets/CASE1.png)
In this case, we can pick a utility of 7c for a \$7 million reward, 5c for a \$5 million reward, and a utility value of 0 for a \$0 reward, where c is a constant. By calculating the utility values, we get $U_{L_A}=1.4c$ and $U_{L_B}=1.25c$. Therefore lottery $L_A$ will be more inclining for individual to go by following the principle of MEU.

However, when we consider the second case:

![Case2](/assets/Case2.png)
If we followed the method above, we will get $3.2c$ on $L_A$ and $2c$ on $L_B$. But an individual will always prefer to have a \$2 million reward with 100% probability instead of \$4 million with 80% probability.

**To sum up**, the utility is not directly proportional to monetary value because the utility for the first million is very high, whereas the utility for an extra million is smaller. 

#### Given a lottery $L-[p, \$X;(1-p),\$Y]$
- The *expected monetary value* is $EMV(L)=pX+(1-p)Y$
- The *utility* is $U(L)=pU(\$X)+(1-p)U(\$Y)$
- Typically, $U(L) < U(EMV(L))$
- In this sense, people will be *risk-averse*.

*For example,* how much would you pay for a lottery ticket $L=[0.5,\$1000;0.5,\$0]$?

For the point where the \$1000 reward probability is 50%, the utility value is lower than the utility of \$500. Here, $400 is the certainty equivalent to the lottery. We will be willing to trade this amount to get the money for certain from the lottery. 
>The difference between these two numbers (expected reward and utility of the lottery) is called **Insurance** or **Risk Premium**.

 ![UtilityCurve](/assets/UtilityCurve.png)

Therefore, a person is willing to take less money with certainty instead of risking a higher proportion. Studies have shown that most people will accept \$400 instead of a gamble that gives \$1000 half the time; the certainty equivalent of the lottery is $400, while the Expected Monetary Value is \$500.